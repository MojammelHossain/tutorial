{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Convolutional Neural Network (CNN)\n",
        "Machine Learning and Deep Learning have been around for decades, and they have evolved progressively through theoretical and practical implementation through trial and error of the theoretical concept. However, there are differences between machine and deep learning model architectures based on data. Assuming you are already familiar with a traditional neural network (NN) known as a multilayer perceptron (MLP), it is the final topic of traditional machine learning and basic concepts before diving into deep learning.\n",
        "\n",
        "The mlp concept is based on human neurons, with each neuron activating based on specific threshold values. However, it has a disadvantage when working with image data because an image must be transformed into a single vector before being sent as input to mlp. Consider a color image with 54 X 54 pixels that are transformed into a single vector, which results in 8748 (54 X 54 X 3) pixels because the color image is composed of three channels (RGB). The amount of trainable weights in a vector after transformation increases considerably, resulting in a huge number of computations based on the number of pixels in an image. Aside from that, the transformation results in spatial information loss, i.e. increased pair pixel distance. To address this issue, a convolutional neural network (CNN) that mimics the human visual cortex has been proposed."
      ],
      "metadata": {
        "id": "ymiIpXxdyWGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Architecture\n",
        "CNN architecture composed of three major building block **convolutional**, **pooling** and **fully connected layer**.\n",
        "\n",
        "**Convolutional Layer (CL):** A single CL is made up of input, filter, and activation functions. A filter, also known as a kernel, is a user-defined matrix that moves across (also known as stride) an input matrix or image from top left to right and bottom. It took the dot product of a single area of the image covered by the kernel and using stride value, the location of the kernel will change until all feasible positions have been covered, i.e. the kernel has gone through all pixel values of the image. Like MLP, the output of the CL will pass through the activation (non-linearity) function, and this activation function is made up of CL. The concept of kernel reduce the number of computation and keep spatial information as selecting a part of the image keep the pixel position data. The kernel concept reduces the number of computations while retaining spatial information by picking a portion of the image and retaining pixel location data. Also, if we take a kernel of size 3 X 3 and train it on an RGB image, the parameter we have to train is 27 (3 X 3 X 3), which is significantly less than the 8748 trainable weights mentioned earlier.\n",
        "\n",
        "**Pooling Layer (PL):** PL functioned similar to CL with minor differences; it reduced the dimension of the output from CL and ensured that subsequent layers could capture the detail of the input. In the CNN architecture, maximum, minimum, and average PL are often utilized.\n",
        "\n",
        "**Fully Connected Layer (FCL):** The FCL is used to classify the output similarly to MLP; we flatten the last CL output into a vector and map that flattened vector to the various classes we have, i.e. mnist classification dataset has 0-9.\n",
        "\n",
        "Consider a face detection task and consider CNN in backward-like backpropagation. The closer the CL is to the output, the more small features like edges and curves are likely to detect, and when we backpropagate from the last layer, we first do it for an edge, and curve, then objects like nose, eyes for face image, and finally layer before input image we can identify it as the face.\n"
      ],
      "metadata": {
        "id": "RzW5NUwFzahe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example\n",
        "In this noteboook I will describe how we can train CNN models using TensorFlow framework."
      ],
      "metadata": {
        "id": "dg67XhKr64Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data collect\n",
        "\n",
        "Collecting data image classification dataset (cat, dog) from github"
      ],
      "metadata": {
        "id": "v9TcwX5O2_qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MojammelHossain/tutorial.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8YH45Cu-qzz",
        "outputId": "f0cbf6b0-c1c2-4db0-e1d4-e75a51436dec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tutorial'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 1), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), 14.33 MiB | 14.48 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/tutorial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzHsfJhO9D3g",
        "outputId": "328317c8-a607-4f8c-8e65-7af38ee7e653"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tutorial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip test.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2WqkfFyBowD",
        "outputId": "628fc391-096e-4184-e1ee-5775ff8ca05e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test.zip\n",
            "   creating: cats/\n",
            "  inflating: cats/cat_190.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_190.jpg  \n",
            "  inflating: cats/cat_147.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_147.jpg  \n",
            "  inflating: cats/cat_542.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_542.jpg  \n",
            "  inflating: cats/cat_595.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_595.jpg  \n",
            "  inflating: cats/cat_422.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_422.jpg  \n",
            "  inflating: cats/cat_583.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_583.jpg  \n",
            "  inflating: cats/cat_384.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_384.jpg  \n",
            "  inflating: cats/cat_586.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_586.jpg  \n",
            "  inflating: cats/cat_545.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_545.jpg  \n",
            "  inflating: cats/cat_223.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_223.jpg  \n",
            "  inflating: cats/cat_551.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_551.jpg  \n",
            "  inflating: cats/cat_587.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_587.jpg  \n",
            "  inflating: cats/cat_140.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_140.jpg  \n",
            "  inflating: cats/cat_342.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_342.jpg  \n",
            "  inflating: cats/cat_430.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_430.jpg  \n",
            "  inflating: cats/cat_418.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_418.jpg  \n",
            "  inflating: cats/cat_395.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_395.jpg  \n",
            "  inflating: cats/cat_156.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_156.jpg  \n",
            "  inflating: cats/cat_585.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_585.jpg  \n",
            "  inflating: cats/cat_234.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_234.jpg  \n",
            "  inflating: cats/cat_355.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_355.jpg  \n",
            "  inflating: cats/cat_433.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_433.jpg  \n",
            "  inflating: cats/cat_341.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_341.jpg  \n",
            "  inflating: cats/cat_332.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_332.jpg  \n",
            "  inflating: cats/cat_468.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_468.jpg  \n",
            "  inflating: cats/cat_124.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_124.jpg  \n",
            "  inflating: cats/cat_118.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_118.jpg  \n",
            "  inflating: cats/cat_520.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_520.jpg  \n",
            "  inflating: cats/cat_290.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_290.jpg  \n",
            "  inflating: cats/cat_119.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_119.jpg  \n",
            "  inflating: cats/cat_88.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_88.jpg  \n",
            "  inflating: cats/cat_496.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_496.jpg  \n",
            "  inflating: cats/cat_523.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_523.jpg  \n",
            "  inflating: cats/cat_251.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_251.jpg  \n",
            "  inflating: cats/cat_279.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_279.jpg  \n",
            "  inflating: cats/cat_244.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_244.jpg  \n",
            "  inflating: cats/cat_60.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_60.jpg  \n",
            "  inflating: cats/cat_446.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_446.jpg  \n",
            "  inflating: cats/cat_268.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_268.jpg  \n",
            "  inflating: cats/cat_255.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_255.jpg  \n",
            "  inflating: cats/cat_109.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_109.jpg  \n",
            "  inflating: cats/cat_525.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_525.jpg  \n",
            "  inflating: cats/cat_281.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_281.jpg  \n",
            "  inflating: cats/cat_94.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_94.jpg  \n",
            "  inflating: cats/cat_313.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_313.jpg  \n",
            "  inflating: cats/cat_1.jpg          \n",
            "  inflating: __MACOSX/cats/._cat_1.jpg  \n",
            "  inflating: cats/cat_528.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_528.jpg  \n",
            "  inflating: cats/cat_306.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_306.jpg  \n",
            "  inflating: cats/cat_56.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_56.jpg  \n",
            "  inflating: cats/cat_106.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_106.jpg  \n",
            "  inflating: cats/cat_113.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_113.jpg  \n",
            "  inflating: cats/cat_96.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_96.jpg  \n",
            "  inflating: cats/cat_473.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_473.jpg  \n",
            "  inflating: cats/cat_116.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_116.jpg  \n",
            "  inflating: cats/cat_464.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_464.jpg  \n",
            "  inflating: cats/cat_114.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_114.jpg  \n",
            "  inflating: cats/cat_538.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_538.jpg  \n",
            "  inflating: cats/cat_504.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_504.jpg  \n",
            "  inflating: cats/cat_5.jpg          \n",
            "  inflating: __MACOSX/cats/._cat_5.jpg  \n",
            "  inflating: cats/cat_358.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_358.jpg  \n",
            "  inflating: cats/cat_417.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_417.jpg  \n",
            "  inflating: cats/cat_371.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_371.jpg  \n",
            "  inflating: cats/cat_575.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_575.jpg  \n",
            "  inflating: cats/cat_574.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_574.jpg  \n",
            "  inflating: cats/cat_158.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_158.jpg  \n",
            "  inflating: cats/cat_564.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_564.jpg  \n",
            "  inflating: cats/cat_203.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_203.jpg  \n",
            "  inflating: cats/cat_375.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_375.jpg  \n",
            "  inflating: cats/cat_162.jpg        \n",
            "  inflating: __MACOSX/cats/._cat_162.jpg  \n",
            "  inflating: cats/cat_18.jpg         \n",
            "  inflating: __MACOSX/cats/._cat_18.jpg  \n",
            "   creating: dogs/\n",
            "  inflating: dogs/dog_147.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_147.jpg  \n",
            "  inflating: dogs/dog_219.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_219.jpg  \n",
            "  inflating: dogs/dog_191.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_191.jpg  \n",
            "  inflating: dogs/dog_344.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_344.jpg  \n",
            "  inflating: dogs/dog_150.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_150.jpg  \n",
            "  inflating: dogs/dog_227.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_227.jpg  \n",
            "  inflating: dogs/dog_421.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_421.jpg  \n",
            "  inflating: dogs/dog_380.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_380.jpg  \n",
            "  inflating: dogs/dog_155.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_155.jpg  \n",
            "  inflating: dogs/dog_141.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_141.jpg  \n",
            "  inflating: dogs/dog_196.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_196.jpg  \n",
            "  inflating: dogs/dog_551.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_551.jpg  \n",
            "  inflating: dogs/dog_237.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_237.jpg  \n",
            "  inflating: dogs/dog_236.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_236.jpg  \n",
            "  inflating: dogs/dog_197.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_197.jpg  \n",
            "  inflating: dogs/dog_168.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_168.jpg  \n",
            "  inflating: dogs/dog_28.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_28.jpg  \n",
            "  inflating: dogs/dog_354.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_354.jpg  \n",
            "  inflating: dogs/dog_142.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_142.jpg  \n",
            "  inflating: dogs/dog_181.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_181.jpg  \n",
            "  inflating: dogs/dog_194.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_194.jpg  \n",
            "  inflating: dogs/dog_369.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_369.jpg  \n",
            "  inflating: dogs/dog_355.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_355.jpg  \n",
            "  inflating: dogs/dog_124.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_124.jpg  \n",
            "  inflating: dogs/dog_130.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_130.jpg  \n",
            "  inflating: dogs/dog_534.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_534.jpg  \n",
            "  inflating: dogs/dog_520.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_520.jpg  \n",
            "  inflating: dogs/dog_521.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_521.jpg  \n",
            "  inflating: dogs/dog_482.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_482.jpg  \n",
            "  inflating: dogs/dog_59.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_59.jpg  \n",
            "  inflating: dogs/dog_327.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_327.jpg  \n",
            "  inflating: dogs/dog_443.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_443.jpg  \n",
            "  inflating: dogs/dog_536.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_536.jpg  \n",
            "  inflating: dogs/dog_244.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_244.jpg  \n",
            "  inflating: dogs/dog_522.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_522.jpg  \n",
            "  inflating: dogs/dog_442.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_442.jpg  \n",
            "  inflating: dogs/dog_89.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_89.jpg  \n",
            "  inflating: dogs/dog_240.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_240.jpg  \n",
            "  inflating: dogs/dog_283.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_283.jpg  \n",
            "  inflating: dogs/dog_123.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_123.jpg  \n",
            "  inflating: dogs/dog_75.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_75.jpg  \n",
            "  inflating: dogs/dog_519.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_519.jpg  \n",
            "  inflating: dogs/dog_518.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_518.jpg  \n",
            "  inflating: dogs/dog_461.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_461.jpg  \n",
            "  inflating: dogs/dog_313.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_313.jpg  \n",
            "  inflating: dogs/dog_528.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_528.jpg  \n",
            "  inflating: dogs/dog_44.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_44.jpg  \n",
            "  inflating: dogs/dog_476.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_476.jpg  \n",
            "  inflating: dogs/dog_462.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_462.jpg  \n",
            "  inflating: dogs/dog_258.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_258.jpg  \n",
            "  inflating: dogs/dog_517.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_517.jpg  \n",
            "  inflating: dogs/dog_43.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_43.jpg  \n",
            "  inflating: dogs/dog_472.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_472.jpg  \n",
            "  inflating: dogs/dog_464.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_464.jpg  \n",
            "  inflating: dogs/dog_302.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_302.jpg  \n",
            "  inflating: dogs/dog_68.jpg         \n",
            "  inflating: __MACOSX/dogs/._dog_68.jpg  \n",
            "  inflating: dogs/dog_114.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_114.jpg  \n",
            "  inflating: dogs/dog_303.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_303.jpg  \n",
            "  inflating: dogs/dog_364.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_364.jpg  \n",
            "  inflating: dogs/dog_563.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_563.jpg  \n",
            "  inflating: dogs/dog_211.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_211.jpg  \n",
            "  inflating: dogs/dog_173.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_173.jpg  \n",
            "  inflating: dogs/dog_415.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_415.jpg  \n",
            "  inflating: dogs/dog_398.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_398.jpg  \n",
            "  inflating: dogs/dog_159.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_159.jpg  \n",
            "  inflating: dogs/dog_213.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_213.jpg  \n",
            "  inflating: dogs/dog_377.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_377.jpg  \n",
            "  inflating: dogs/dog_177.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_177.jpg  \n",
            "  inflating: dogs/dog_229.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_229.jpg  \n",
            "  inflating: dogs/dog_360.jpg        \n",
            "  inflating: __MACOSX/dogs/._dog_360.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "ZmRiE4b33Q8g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mX2MpaNr8yVZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical, Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader\n",
        "We will create a dataset class using the TensorFlow Sequence class to generate data batches for training the model.\n",
        "\n",
        "In this example, we will read the image using a function read_img() that takes the image path as an argument and read using opencv library. We will resize our image so that each image follows the same dimension which is necessary for batching the data."
      ],
      "metadata": {
        "id": "V8xggFjG3oUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 8\n",
        "NUM_CLASS = 2\n",
        "IN_CHANNELS = 3\n",
        "\n",
        "def read_img(path):\n",
        "    \"\"\"\n",
        "    Summary:\n",
        "        read, resize and normalize an image given a path\n",
        "    Arguments:\n",
        "        path (string): image path\n",
        "    Return:\n",
        "        numpy array\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, IMG_SIZE)\n",
        "    return cv2.normalize(img, None, 0, 1.0, cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "class MyDataset(Sequence):\n",
        "\n",
        "    def __init__(self, img_dir, tgt, in_channels, batch_size, num_class):\n",
        "        \n",
        "        \"\"\"\n",
        "        Summary:\n",
        "            initialize class variables\n",
        "        Arguments:\n",
        "            img_dir (list): all image directory\n",
        "            tgt (list): corresponding image label\n",
        "            in_channels (int): number of input channels\n",
        "            batch_size (int): how many data to pass in a single step\n",
        "            num_class (int): number of class in mask image\n",
        "        Return:\n",
        "            class object\n",
        "        \"\"\"\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.tgt = tgt\n",
        "        self.in_channels = in_channels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        \"\"\"\n",
        "        return total number of batch to travel full dataset\n",
        "        \"\"\"\n",
        "        \n",
        "        return math.ceil(len(self.img_dir) // self.batch_size)  # get the total number of batch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        \"\"\"\n",
        "        Summary:\n",
        "            create a single batch for training\n",
        "        Arguments:\n",
        "            idx (int): sequential batch number\n",
        "        Return:\n",
        "            images and masks as numpy array for a single batch\n",
        "        \"\"\"\n",
        "        \n",
        "        # get a single batch for given idx. Ex: for idx=0, batch[0:batch_size] again for idx=1, batch[batch_size:2*batch_size]\n",
        "        batch_x = self.img_dir[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
        "        batch_y = self.tgt[idx * self.batch_size:(idx + 1) *self.batch_size]\n",
        "\n",
        "        imgs = []\n",
        "        tgts = []\n",
        "        \n",
        "        for i in range(len(batch_x)):   # get all image and target for single batch\n",
        "            imgs.append(read_img(batch_x[i]))\n",
        "            tgts.append(to_categorical(batch_y[i], num_classes = self.num_class))\n",
        "\n",
        "        # converting list to numpy array\n",
        "        tgts = np.array(tgts)\n",
        "        imgs = np.array(imgs)        \n",
        "        return tf.convert_to_tensor(imgs), tf.convert_to_tensor(tgts)   # return non-weighted images and targets"
      ],
      "metadata": {
        "id": "i-Z_G68jDKgX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader\n",
        "We will create dataloader object using the dataset class written above to pass through the model. Also partition the dataset into train, valid, and test."
      ],
      "metadata": {
        "id": "SGLxLNfs8fgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_val_test_dataloader(path, label):\n",
        "    \n",
        "    \"\"\"\n",
        "    Summary:\n",
        "        read train and valid image and mask directory and return dataloader\n",
        "    Arguments:\n",
        "        config (dict): Configuration directory\n",
        "    Return:\n",
        "        train and valid dataloader\n",
        "    \"\"\"\n",
        "    x_train, x_rem, y_train, y_rem = train_test_split(path, label, train_size = 0.75, random_state=42)\n",
        "    x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size = 0.5, random_state=42)\n",
        "\n",
        "    print(\"train Example : {}\".format(len(x_train)))\n",
        "    print(\"valid Example : {}\".format(len(x_valid)))\n",
        "    print(\"test Example : {}\".format(len(x_test)))\n",
        "\n",
        "    train_dataloader = MyDataset(x_train,\n",
        "                              y_train,\n",
        "                              in_channels = IN_CHANNELS,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              num_class = NUM_CLASS)\n",
        "\n",
        "    # create dataloader object for validation dataset\n",
        "    val_dataloader = MyDataset(x_valid,\n",
        "                            y_valid,\n",
        "                            in_channels = IN_CHANNELS,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            num_class = NUM_CLASS)\n",
        "    test_dataloader = MyDataset(x_test,\n",
        "                            y_test,\n",
        "                            in_channels = IN_CHANNELS,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            num_class = NUM_CLASS)\n",
        "    \n",
        "    return train_dataloader, val_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "xfarTC_aBrhG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "Using the glob library we will fetch the image paths and generate labels. We consider cat as 0 and dog as 1 further we will initialize our train, valid, and test dataloader.\n"
      ],
      "metadata": {
        "id": "DOIAp2_Z9FmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch image paths\n",
        "cat_dir = glob.glob(\"cats/*.*\")\n",
        "dogs_dir = glob.glob(\"dogs/*.*\")\n",
        "\n",
        "# generate labels\n",
        "label = []\n",
        "for i in range(len(cat_dir)):\n",
        "  label.append(0)\n",
        "for i in range(len(dogs_dir)):\n",
        "  label.append(1)\n",
        "\n",
        "# create python dataframe and shuffle data\n",
        "data_dir = cat_dir + dogs_dir\n",
        "df = pd.DataFrame(np.array([data_dir, label]).T, columns=[\"path\", \"label\"]).sample(frac=1, random_state=42).reset_index()\n",
        "df[\"label\"] = pd.to_numeric(df[\"label\"])\n",
        "\n",
        "# initialize dataloader object\n",
        "train_data, val_data, test_data = get_train_val_test_dataloader(list(df['path']), list(df[\"label\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV2KYcDTXNgL",
        "outputId": "a91eb810-7df5-429a-86d4-f1080ab2fc8c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train Example : 105\n",
            "valid Example : 17\n",
            "test Example : 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "\n",
        "Over the year different CNN architecture has been proposed for different tasks. However, we will create our own CNN architecture. As previously describe CNN is composed of three major layers so we will use those layers to create our architecture. in Tensorflow CL known as Conv2D, for PL we will be using MaxPooling2D which chooses the max value from a sub-part of the image, and FCL known as Dense."
      ],
      "metadata": {
        "id": "MF-aa2-S-A98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model():\n",
        "    \"\"\"\n",
        "    Summary:\n",
        "        create a define CNN model\n",
        "    Return:\n",
        "        return a keras model object\n",
        "    \"\"\"\n",
        "    input = tf.keras.layers.Input((IMG_SIZE[0], IMG_SIZE[1], IN_CHANNELS))\n",
        "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(input)\n",
        "    c1 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    flat = tf.keras.layers.Flatten()(p2)\n",
        "    d1 = tf.keras.layers.Dense(256, activation='relu')(flat)\n",
        "    d2 = tf.keras.layers.Dense(128, activation='relu')(d1)\n",
        "    out = tf.keras.layers.Dense(2, activation='softmax')(d2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input], outputs=[out])\n",
        "    return model"
      ],
      "metadata": {
        "id": "YIkCiRFQegQX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model initialization\n",
        "\n",
        "Let us initialize our model and see how many parameter/weights we need to train."
      ],
      "metadata": {
        "id": "xZ2QmpgV-_I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsiyTaxFhNrc",
        "outputId": "554e711b-3bd6-4c77-c9a4-6f75b8d2cc19"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 128, 128, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 64, 64, 64)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 128)       73856     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 64, 64, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 32, 32, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 131072)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33554688  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,848,002\n",
            "Trainable params: 33,848,002\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "To train a TensorFlow model we need compile our model first and define some important arguments.\n",
        "\n",
        "Loss: As we are doing binary classification so we will be using BinaryCrossentropy loss function which is responsible for penalizing bad prediction.\n",
        "\n",
        "Metrics: It is a performance score to measure how much our model predictions are accurate. Here we will use the accuracy metric.\n",
        "\n",
        "Now let us train our model for 10 epoch."
      ],
      "metadata": {
        "id": "7ZU7rKhn_Mj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_data,\n",
        "                    verbose = 1, \n",
        "                    epochs = 10,\n",
        "                    validation_data = val_data, \n",
        "                    shuffle = False,\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7zCV9S4hQtl",
        "outputId": "f47e2e7d-5a82-4cfb-fb67-4c5281662ae0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "13/13 [==============================] - 38s 3s/step - loss: 6.3185 - accuracy: 0.4423 - val_loss: 0.6976 - val_accuracy: 0.4375\n",
            "Epoch 2/10\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.7162 - accuracy: 0.4808 - val_loss: 0.7357 - val_accuracy: 0.4375\n",
            "Epoch 3/10\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.6923 - accuracy: 0.5385 - val_loss: 0.7022 - val_accuracy: 0.4375\n",
            "Epoch 4/10\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.6813 - accuracy: 0.6250 - val_loss: 0.7187 - val_accuracy: 0.4375\n",
            "Epoch 5/10\n",
            "13/13 [==============================] - 35s 3s/step - loss: 0.5597 - accuracy: 0.7981 - val_loss: 1.1898 - val_accuracy: 0.4375\n",
            "Epoch 6/10\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.4705 - accuracy: 0.7692 - val_loss: 1.0109 - val_accuracy: 0.6250\n",
            "Epoch 7/10\n",
            "13/13 [==============================] - 36s 3s/step - loss: 0.4508 - accuracy: 0.8558 - val_loss: 0.8068 - val_accuracy: 0.5625\n",
            "Epoch 8/10\n",
            "13/13 [==============================] - 44s 3s/step - loss: 0.3738 - accuracy: 0.8558 - val_loss: 1.6678 - val_accuracy: 0.5625\n",
            "Epoch 9/10\n",
            "13/13 [==============================] - 48s 4s/step - loss: 0.1997 - accuracy: 0.9519 - val_loss: 1.9122 - val_accuracy: 0.5625\n",
            "Epoch 10/10\n",
            "13/13 [==============================] - 47s 4s/step - loss: 0.0761 - accuracy: 0.9712 - val_loss: 5.2372 - val_accuracy: 0.4375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "Now we will test our trained model on our test data to evaluate its performance in unseen data."
      ],
      "metadata": {
        "id": "5X1MfgVpApo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i4XsgTXh3uM",
        "outputId": "9cf542bf-e31f-4c8a-c527-88ac80e21702"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 2s 654ms/step - loss: 5.1186 - accuracy: 0.4375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.118557929992676, 0.4375]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Application\n",
        "We can see that our model can identify the training data with 97% accuracy; however, in both validation and test data it can identify only 43.75% of the data correctly which could be for various reasons in which one is overfitting on train data which can be reduced by increasing the number data.\n",
        "\n",
        "CNN architecture has different application\n",
        "\n",
        "*  Object detection\n",
        "*   Tracking vehicles\n",
        "*  Identifying Flood casuality from Satelite image\n",
        "\n"
      ],
      "metadata": {
        "id": "qt9mnkYlA6Kl"
      }
    }
  ]
}